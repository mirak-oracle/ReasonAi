<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ReasonAI - Voice Interaction</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f8f9fa;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }

        .container {
            text-align: center;
        }

        h1 {
            color: #007bff;
            margin-bottom: 20px;
        }

        p {
            color: #495057;
            margin-bottom: 20px;
        }

        #output {
            margin-bottom: 20px;
            font-size: 18px;
            font-weight: bold;
            color: #333;
        }

        button {
            padding: 10px 20px;
            background-color: #007bff;
            color: #fff;
            border: none;
            border-radius: 4px;
            font-size: 16px;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }

        button:hover {
            background-color: #0056b3;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ReasonAI - Voice Interaction</h1>
        <p>Speak to ReasonAI and ask questions to engage in reasoning.</p>
        <div id="output"></div>
        <button onclick="startListening()">Start Listening</button>
    </div>

    <script>
        const recognition = new window.webkitSpeechRecognition();
        recognition.continuous = false;
        recognition.lang = 'en-US';

        recognition.onresult = async function(event) {
            const transcript = event.results[0][0].transcript;
            document.getElementById('output').innerText = 'You said: ' + transcript;

            // Process speech input using NLP
            await processSpeech(transcript);
        };

        recognition.onerror = function(event) {
            console.error('Speech recognition error:', event.error);
        };

        async function processSpeech(input) {
            // Send speech input to NLP service for analysis
            const response = await fetch('https://nlp-service.com/analyze', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({ input: input })
            });

            // Parse NLP response
            const nlpData = await response.json();

            // Extract intent and entities from NLP response
            const intent = nlpData.intent;
            const entities = nlpData.entities;

            // Determine response based on intent and entities
            let responseText;
            switch (intent) {
                case 'get_weather':
                    const location = entities.location;
                    responseText = `The weather in ${location} is currently sunny.`;
                    break;
                case 'search_query':
                    const query = entities.query;
                    // Perform search or query processing
                    responseText = `Here are the search results for "${query}".`;
                    break;
                default:
                    responseText = `I'm sorry, I didn't understand that.`;
            }

            // Speak or display response
            speakResponse(responseText);
        }

        function startListening() {
            recognition.start();
            document.getElementById('output').innerText = 'Listening...';
        }

        function speakResponse(response) {
            const synthesis = window.speechSynthesis;
            const utterance = new SpeechSynthesisUtterance(response);
            synthesis.speak(utterance);
        }
    </script>
</body>
</html>
